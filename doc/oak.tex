\documentclass[a4paper,12pt]{article}
\usepackage{fullpage,color}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{amsmath}
\newcommand{\sgn}{\mathop{\mathrm{sgn}}}

\definecolor{mylinkcolor}{rgb}{.0,.0,.0} 
\definecolor{section2}{rgb}{0.188235,0.011765,0.611765}

\hypersetup{
    pdfborder=0,%
    colorlinks,%
    citecolor=mylinkcolor,%
    anchorcolor=mylinkcolor,%
    linkcolor=mylinkcolor,%
    urlcolor=section2
}


\newcommand{\code}{\texttt}
\newcommand{\mat}{\mathbf}
\renewcommand{\vec}{\mathbf}
\newenvironment{codeexample}{\begin{verbatim}}{\end{verbatim}}
\newenvironment{keytabular}{\begin{tabular}{|p{0.3\textwidth}|p{0.2\textwidth}|p{0.5\textwidth}|} \hline Key & Type & Description \\ \hline \hline }{\end{tabular}}
\newcommand{\subsubsubsection}[1]{\bigskip \textit{#1} \medskip}
%\newcommand{\subsubsubsection}[1]{\paragraph{#1}}

\definecolor{color_nesting}{rgb}{.3,0,1}
\begin{document}


\setlength{\parindent}{0cm}
\newcommand{\tabularcol}{|p{0.3\textwidth}|p{0.2\textwidth}|p{0.5\textwidth}|}

\title{Ocean Assimilation Kit (OAK) \\ User guide}
\author{Alexander Barth, Luc Vandenbulcke}
\maketitle


%\chapter{Users guide}

\section{Structure of the software}

The software is structured in different modules

\begin{itemize}

\item \code{ufileformat:} Binary output and input of large 1D, 2D or 3D
matrices in the GHER or NetCDF.
\item \code{initfile:} Input of integers, floating numbers, strings and small
vectors of those data types.
\item \code{matoper:} Basic matrix operating: multiplication, matrix
  inversion, eigenvalue/eigenvectors and singular value decomposition
  (relying on BLAS and LAPACK).
\item \code{date:} module for conversion between modified Julian day number
  and Gregorian date.
\item \code{grids:} interpolation from one grid to another of 1D, 2D or
  3D data.
\item \code{rrsqrt:} The analysis equation
\item \code{assimilation:} I/O of state vector, observations, error
  space and observation operator. Analysis routine with input/output
  and computation of diagnostics.
\end{itemize}

These modules can be either used for specific task with standalone programs \ref{sec_standalone} or by a hydrodynamic model
in the case of a simulation assimilating observations. The GHER
hydrodynamic model drives the data assimilation modules trough the
following subroutines:

\begin{itemize}
\item \code{dainit:} initialises of the data assimilation modules
\item \code{daobs:} loads of the next observation to assimilate
\item \code{daanalysis:} performs the analysis
\item \code{damoderr:} propagates the error covariance of the model 
\end{itemize}

\section{Module: \code{ufileformat}}\label{subsec_use_ufileformat}

This module is used for binary output and input of large 1D, 2D or 3D
matrices. The GHER and a subset of the NetCDF format is currently
supported. The matrix can contain exclusion points
(``holes''). Matrices $A$ where the elements are a linear combination of
the indices can also be efficiently represented: 

\begin{equation}
A(i,j,k) = a_0 + a_1 i + a_2 j + a_3 k
\end{equation}

Only the coefficient $a_0$, $a_1$, $a_2$ and $a_3$ are
stored. These file are called degenerated. For example, the longitude and latitude of each grid point can
often be expressed in this way.\\

For the GHER format, each file represent a real matrix. If the
file names ends with \code{.gz}, then the file is uncompressed
(with \code{gunzip}) in the user's temporary directory defined by the
environment variable \code{\$TMPDIR} (or by default in
\code{/tmp}). Simple Fortran 90-style extraction can also by performed
with the module \code{ufileformat}. A coma separated
list of indices or ranges of indices in parenthesis can be appended to
the file name, if only a subsection of the matrix should be loaded.

For example if the file
\code{toto.TEM} is a 10 x 10 x 10 matrix, the ``file'':

\begin{description}
\item[\code{toto.TEM(:,:,6)}] is 10x10x1 matrix containing all
  elements with the 3rd indices equal to 6.
\item[\code{toto.TEM(:,end,:)}] is 10x1x10 matrix containing all
  elements with the 2nd indices equal to 10.
\item[\code{toto.TEM(1:,:end,1:end)}] is 10x10x10 matrix equal to the
  original matrix
\end{description}

But no arithmetic with the indices (for example
\code{toto.TEM(:,end-1,:)}) are allowed. If data extraction is used
with degenerated matrices, the four coefficient are changed
accordingly to the subsection chosen.\\

Data extraction and automatic decompression can only be used
for loading data.  \\

A variable in a NetCDF file can be loaded by specifying a ``file name'' of
the following form:

\begin{verbatim}
NetCDF_filename#NetCDF_variable
\end{verbatim}

If the NetCDF file name end with \code{.gz}, then the file is uncompressed
as with the GHER file format. The data extraction follows also the same
rules as above. For example, the following is a valid file name for
loading a matrix. \\

\begin{verbatim}
file.nc.gz#temp(:,:,1)
\end{verbatim}

The file \code{file.nc.gz} is first decompressed, then the slice with
the 3rd indices equal to 1 of the variable \code{temp} is returned to
the calling program. \\
 
The special value for missing data is stored in the variables attribute
\code{missing\_data}. In the case of degenerated file, the attribute
\code{shape} must be present, containing the shape of the matrix. The actual value
of the variable contains the coefficients $a_i$.

\subsection{Order of the dimensions}

The reported order of the dimensions depends on the tool that you are using to query and access a file. Two types of \href{http://en.wikipedia.org/wiki/Row-major\_order}{ordering schemes} exists:

\begin{description}
\item[column-major order]: used by Fortran programs such as OAK
\item[row-major order]: used by C programs such as ncdump
\end{description}

The order of the dimensions for NetCDF follows the recommendation of the  \href{http://cf-pcmdi.llnl.gov/documents/cf-conventions/1.6/cf-conventions.html\#dimensions}{CF-convention}. If you query your NetCDF files with ncdump, the order of the dimensions should be time, depth, latitude, longitude. For a Fortran program reading this file the dimensions with automatically be longitude, latitude, depth and time since Fortran uses the column-major order (as opposed to ncdump). For Fortran binary files, the order of the dimensions is also longitude, latitude, depth and time.

\section{The initialisation file}

With the module \code{initfile} a program can read an integer number,
floating number or a character string from an initialisation file.  Each line in
this file is composed by a name (called key), an equal sign and the value. For
example:

\begin{verbatim}
runtype = 2
Geoflow.maxU = 0.3 
logfile = 'assim.log' 
\end{verbatim} 

When the program search for example the key ``\code{runtype}'', it gets
the integer 2. If a key is present several times in the same
initialisation file, then the last value found is taken. \\

The key can be composed by any alphanumeric character and by periods (.).
In particular, spaces and a equal signs are not allowed within the key
name. The wild cards symbols *, ? and brackets ([,]), are allowed but have
a special meaning (see Paragraph below). \\

Vectors of integers, floats and character strings are also supported. The
values are separated with commas and enclosed in brackets.

\begin{verbatim}
Model.variables = ['ETA','TEM','SAL'] 
Model.maxCorrection = [0.3,3.,2.,0.3,3.,2.,0.3,3.,2.] 
\end{verbatim} 

Blank lines are ignored and comments begin with the pound sign (\#).
It is recommended to document the meaning and the possible values
by a comments directly in the initialisation file. \\

Entries in this files cannot be split across different lines. Before
assigning a value to a key you should know with type is expected:
scalar or vector and number or characters. If the type does not
correspond, the program will be stopped. \\

Sometimes a sequence of keys are attributed to the same values:

\begin{verbatim}
Obs001.path      = '/u/abarth/soft/Ligur3/Obs/' 
Obs002.path      = '/u/abarth/soft/Ligur3/Obs/' 
Obs003.path      = '/u/abarth/soft/Ligur3/Obs/' 
\end{verbatim} 

In this case one can use wild cards and write the following:

\begin{verbatim}
Obs*.path      = '/u/abarth/soft/Ligur3/Obs/' 
\end{verbatim} 

The meaning of the wild cards are the same as for file name generation of the
Burne Shell (see also man page of sh and gmatch).  

\section{Assimilation module}

\subsection{Reduced order analysis}

Let $N$ be the ensemble size, $n$ the size of the state vector and $m$ the observation space dimension. The best linear unbiased estimator (BLUE) of the model's state vector
given the model forecast $\mathbf x^f$ with error covariance $\mathbf
P^f$ and the observation $\mathbf y^o$ with error covariance $\mathbf
R$ is given by $\mathbf x^a$:

\begin{eqnarray}
\mathbf x^a &=& \mathbf x^f + \mathbf K 
\left(\mathbf y^o - \mathbf H \mathbf x^f \right) \\
\mathbf{K} &=& \mathbf P^f \mathbf H^T 
\left( \mathbf H \mathbf P^f \mathbf H^T + \mathbf R \right)^{-1} 
\label{KPf} \\
\mathbf P^a &=&
\mathbf P^f - \mathbf K \mathbf H \mathbf P^f \label{PaPf}
\end{eqnarray}

where $\mathbf H$ is the observation operator extracting the observed
part of the state vector and $\mathbf P^a$ the error covariance of the
analysis $\mathbf x^a$. 

From the ensemble of forecast states $\mathbf {x^f}^{(k)}$ where $k=1,\dots,N$ one can compute the ensemble mean 

\begin{equation}
\overline{\mathbf x^{f}}=\frac{1}{N}\sum_{k=1}^N \mathbf x^{f^{(k)}} %\label{ensmean}
\end{equation}

and ensemble covariance:

\begin{equation}
\mathbf P^f =\frac{1}{N-1} \sum_{k=1}^N 
\left( \mathbf x^{f^{(k)}} - \overline{\mathbf x^{f}} \right) 
\left( \mathbf x^{f^{(k)}} - \overline{\mathbf x^{f}} \right)^T 
\end{equation}

We construct the columns of the matrix $\mathbf S^f$ by:

\begin{equation}
\left( \mathbf{S}^f \right)_k =\frac{{\mathbf{x}^f}^{(k)}-\overline{\mathbf{x}^f}}{\sqrt{N-1}} \label{Saf}
\end{equation}

where $\mathbf S^f$ is a $n \times N$ matrix, which each column being the difference between each member its ensemble mean.
Its mean over all columns it thus zero.
As many other assimilation schemes (SEEK, RRSQRT, ESSE, EnKF), $\mathbf P^f$ is decomposed in terms of this square root 
matrix $\mathbf S^f$:
\begin{equation}
\mathbf P^f = \mathbf S^f {\mathbf S^f}^T   \label{PfSf}
\end{equation}


%\begin{equation}
%\overline{\mathbf x^{a,f}}=\frac{1}{N}\sum_{k=1}^N \mathbf x^{a,f^{(k)}} \label{ensmean}
%\end{equation}

Typically, the number of ensemble members $N$ is much smaller than the state vector size $n$. We rewrite the Kalman Filter analysis, by avoiding any matrix of the size $n \times n$:

% be respectively the analysis (a) and forecast (f)  ensemble mean.
% The reduced order
% implementation is only effective if $N$ is small ($N << n$). Furthermore
% $\mathbf R$ must be diagonal for efficiency. Basically what we want to do is express $\mathbf{P}^a$ as a product of $\mathbf{S}^a{\mathbf{S}^a}^T$ as well.
% \\
%Let's  first calculate the Kalman gain in order to reuse it in \eqref{PaPf}. Injecting \eqref{PfSf} in \eqref{KPf}, one can write

\begin{eqnarray}
\mathbf K  
&=& 
(\mathbf S ^f {\mathbf S ^f}^T) \mathbf H ^T \left[\mathbf H (\mathbf S^f  {\mathbf S^f}^T) \mathbf H^T + \mathbf R  \right]^{-1}\label{beforeSherman}\\
&=& 
\mathbf S ^f (\mathbf H \mathbf S ^f)^T  \left[ \mathbf H \mathbf S^f(\mathbf H \mathbf S^f)^T+\mathbf R  \right]^{-1}\label{beforeSherman}\\
&=&
\mathbf S^f\left[\mathbf I + (\mathbf H \mathbf S^f)^T\mathbf R^{-1} \mathbf H \mathbf S^f \right]^{-1}(\mathbf H \mathbf S^f)^T\mathbf R^{-1}\label{afterSherman}
\end{eqnarray}

Where the Sherman-Morison-Woodbury identity has been applied from \eqref{beforeSherman} to \eqref{afterSherman}. This identity can be expressed as:

\begin{equation}
\mathbf{AB}^T{\left(\mathbf{C}+\mathbf{BAB}^T\right)}^{-1} = 
{\left(\mathbf{A}^{-1}+\mathbf{B}^T\mathbf{C}^{-1}\mathbf{B}\right)}^{-1}\mathbf{B}^T\mathbf{C}^{-1}
\end{equation}

with $\mathbf A = \mathbf I$, $\mathbf B=\mathbf{HS}^f$, $\mathbf{C}=\mathbf{R}$.
That is, instead of performing the inverse in space of matrix $\mathbf{A}$ the inverse is done in the space of the matrix $\mathbf C$. We also substitute $\mathbf P^f$ in the expression of the analysis covariance error $\mathbf P^a$: \newline

\begin{eqnarray}
\mathbf{P}^a
&=&
\mathbf P^f - \mathbf K \mathbf H \mathbf P^f \\
&=&
\mathbf S^f {\mathbf S^f}^T - \mathbf K \mathbf H \mathbf S^f {\mathbf S^f}^T \\
&=&
\mathbf S^f {\mathbf S^f}^T
-
{\color{blue}\mathbf S^f\left[\mathbf I + (\mathbf H \mathbf S^f)^T\mathbf R^{-1} \mathbf H \mathbf S^f\right]^{-1}(\mathbf H \mathbf S^f)^T\mathbf R^{-1}}
\mathbf H \mathbf S {\mathbf S^f}^T \\
&=&
\mathbf S^f 
\left[ 
\mathbf I
-
\left(\mathbf I + (\mathbf H \mathbf S^f)^T\mathbf R^{-1} \mathbf H \mathbf S^f\right)^{-1}(\mathbf H \mathbf S^f)^T\mathbf R^{-1}
\mathbf H \mathbf S 
\right] {\mathbf S^f}^T  \label{eqn:Pa_with_Sf}
\end{eqnarray}

In order to avoid to form $\mathbf P^a$ explicitly, we need to express $\mathbf P^a$ also in terms of the square root matrices $\mathbf S^a$.

\begin{equation}
 \mathbf P^a = \mathbf S^a {\mathbf S^a}^T
\end{equation}

This is possible when the following eigenvalue decomposition is made :

\begin{equation}
\left(\mathbf H \mathbf S^f\right)^T  \mathbf R^{-1} \mathbf H \mathbf S^f
= 
\mathbf U \mathbf \Lambda \mathbf U^T \label{Udec}
\end{equation} 
where $\mathbf U^T \mathbf U = \mathbf I$ and where $\mathbf \Lambda$ is
diagonal.
 $\mathbf U$ and $\mathbf \Lambda$ are both of the size $N \times N$.\newline  

Using the decomposition \eqref{Udec} in equation (\ref{eqn:Pa_with_Sf}) one obtains:
% \eqref{afterSherman} and injecting \eqref{afterSherman} and \eqref{PfSf} in \eqref{PaPf} leads to

\begin{eqnarray}
\mathbf{P}^a
&=&
\mathbf S^f
\left[
\mathbf I - 
(\mathbf I+ \mathbf U \mathbf \Lambda \mathbf U^T )^{-1}\mathbf U \mathbf \Lambda \mathbf U^T
\right]
{\mathbf S^f}^T\\
&=&
\mathbf S^f
\left[
\mathbf I - 
(\mathbf I+ \mathbf U \mathbf \Lambda \mathbf U^T )^{-1}
\left( \mathbf U \mathbf \Lambda \mathbf U^T + \mathbf I - \mathbf I\right) 
\right]
{\mathbf S^f}^T\\
&=&
\mathbf S^f
\left[
\mathbf I - 
(\mathbf I+ \mathbf U \mathbf \Lambda \mathbf U^T )^{-1}
\left( \mathbf U \mathbf \Lambda \mathbf U^T + \mathbf I \right) 
 +
(\mathbf I+ \mathbf U \mathbf \Lambda \mathbf U^T )^{-1}
\right]
{\mathbf S^f}^T\\
&=&
\mathbf S^f
\left[
\mathbf I - 
\mathbf I 
 + 
(\mathbf I+ \mathbf U \mathbf \Lambda \mathbf U^T )^{-1}
\right]
{\mathbf S^f}^T \\
&=&
\mathbf S^f
(\mathbf I+ \mathbf U \mathbf \Lambda \mathbf U^T )^{-1}
{\mathbf S^f}^T \\
&=&
\mathbf S^f
(\mathbf U \mathbf U^T+ \mathbf U \mathbf \Lambda \mathbf U^T )^{-1}
{\mathbf S^f}^T \\
&=&
\mathbf S^f \mathbf U
(\mathbf I + \mathbf \Lambda )^{-1}
\mathbf U^T {\mathbf S^f}^T \\
&=&
\mathbf S^f \mathbf U
(\mathbf I + \mathbf \Lambda )^{-1/2}
(\mathbf I + \mathbf \Lambda )^{-1/2}
\mathbf U^T {\mathbf S^f}^T \label{PaSfSft}
\end{eqnarray}

So we found a square root decomposition of $\mathbf{P}^a$ in terms of $\mathbf{S}^f\mathbf{U}(\mathbf I + \mathbf \Lambda)^{-1/2}$. But in order to construct an ensemble from the columns of $\mathbf S^a$, its mean has to be zero. So we will transform $\mathbf S^a$ so that the identity \eqref{PaSfSft} is preserved. One way to do this is

\begin{equation}
   \mathbf S^a = \mathbf S^f \mathbf U (\mathbf I + \mathbf \Lambda)^{-1/2} \mathbf U^T  %\mathbf \Omega
\end{equation}


The decomposition (\ref{Udec}) can also be used in the computation of the Kalman gain $\mathbf K$:
by:

\begin{eqnarray}
 \mathbf K 
&=&
\mathbf S^f
\left[
\mathbf I + (\mathbf H \mathbf S^f)^T\mathbf R^{-1} \mathbf H \mathbf S^f 
\right]^{-1}
(\mathbf H \mathbf S^f)^T\mathbf R^{-1} \\
&=&
\mathbf S^f
\left[
\mathbf U \mathbf U^T + \mathbf U \mathbf \Lambda \mathbf U^T
\right]^{-1}
(\mathbf H \mathbf S^f)^T\mathbf R^{-1} \\
&=&
 \mathbf S^f \mathbf U (\mathbf I + \mathbf \Lambda)^{-1} \mathbf U^T (\mathbf H \mathbf S^f)^T \mathbf R^{-1}\label{KU}
\end{eqnarray}

\newcommand{\one}{ \mathbf 1_{N \times 1}}

% $\mathbf \Omega$ is an orthogonal matrix chosen such that sum of all columns of $\mathbf S^a$ is zero. This sum can be obtained by multiplying to the right with a vector $N \times 1$ with all elements equal to 1 ($\one$). Since $\mathbf S^f \one$ is zero, $\mathbf S^a \one$ is also zero if:

% \begin{eqnarray}
%   \mathbf U (\mathbf I + \mathbf \Lambda)^{-1/2} \mathbf U^T \mathbf \Omega \, \one &=& \one \\
%   \mathbf \Omega \, \one &=& \mathbf U (\mathbf I + \mathbf \Lambda)^{1/2} \mathbf U^T \one
% \end{eqnarray}

% We defined the normalised vectors $\mathbf w$ and $\mathbf v$:

% \begin{eqnarray}
%   \mathbf w &=& \frac{1}{\sqrt{N}} \one \\
%   \mathbf v &=& \alpha \mathbf U (\mathbf I + \mathbf \Lambda)^{1/2} \mathbf U^T \one
% \end{eqnarray}

% $\mathbf \Omega$ is thus a matrix which rotates $\mathbf w$ onto $\mathbf v$. 
% \begin{equation}
% \mathbf \Omega \mathbf w = \mathbf v
% \end{equation}

% It can be computed by:

% \begin{eqnarray}
% \mathbf \Omega = \mathbf v \mathbf w^T  + \mathbf H({\mathbf v}) \mathbf H({\mathbf w})^T
% \end{eqnarray}

% where $\mathbf H(\mathbf v)$ is the $N \times (N-1)$ Householder matrix associated to the vector $\mathbf v$, i.e.
% all columns of $\mathbf H({\mathbf v})$ are vectors perpendicular to $\mathbf v$ \citep{Hoteit02}:

% \begin{eqnarray}
% \mathbf H(\mathbf v)_{i,j} &=& \delta_{i,j} - \frac{\mathbf v_i \mathbf v_j}{|\mathbf v_N| + 1}   \qquad \mbox{for $i \le N-1$} \\
% \mathbf H(\mathbf v)_{N,j} &=& - \frac{(\mathbf v_N + \sgn(\mathbf v_N)) \mathbf v_j }{|\mathbf v_N| + 1}
% \end{eqnarray}

%% \begin{equation*}
%% \mathbf H(\mathbf v)_{i,j} = 
%% \begin{cases}
%%   \delta_{i,j} - \frac{v_i v_j}{|v_r| + 1}    & \text{if } j < r, \\
%%   - \frac{(v_r + \sgn(v_r)) v_j }{|v_r| + 1} & \text{if } j = r. 
%% \end{cases}
%% \end{equation*}



%% \begin{equation*}
%% \sgn(x) = \begin{cases}
%% -1 & \text{if } x < 0, \\
%% 0 & \text{if } x = 0, \\
%% 1 & \text{if } x > 0. \end{cases}
%% \end{equation*}

%% alpha = - 1/(abs(v(n))+1);

%% do j=1,n-1
%%   do i=1,n
%%     H(i,j) = alpha * v(i)*v(j);
%%     if (i.eq.j) then
%%       H(i,j) = H(i,j)+1;
%%     end if
%%   end do
%% end do

%% do j=1,n-1
%%     H(n,j) = alpha * (v(n)+sign(1.,v(n)))*v(j);


{%\color{blue}
For a linear observation operator, the sum of all columns of $\mat H \mat S^f$ is zero. Thus $\one$ is a (unnormalized) eigenvector of $\left(\mathbf H \mathbf S^f\right)^T  \mathbf R^{-1} \mathbf H \mathbf S^f$ with eigenvalue 0:


\begin{equation}
\left(\mathbf H \mathbf S^f\right)^T  \mathbf R^{-1} \mathbf H \mathbf S^f
\one 
= 
0 \;
\one
\end{equation}

If eigenvalues are sorted in $\mat \Lambda$, then $\one$ is the smallest and last eigenvalue (as all eigenvalues positive).


\begin{eqnarray}
  \mat U \vec e_N &=& \frac{1}{\sqrt{N}} \one \\
  \mat U^T \frac{1}{\sqrt{N}} \one &=& \vec e_N
\end{eqnarray}

where $\vec e_N$ is the a vector with all elements equal to zero expect that last which is one. Therefore, it follows that

\begin{equation}
  \mathbf U (\mathbf I + \mathbf \Lambda)^{-1/2} \mathbf U^T \one = \one
\end{equation}

since the element $\mat \Lambda_{N,N}$ is zero. Thus the mean of all columns of $\mat S^a$ is zero.
%or a linear observation operator, the matrix $\mat \Omega$ above is thus the identity matrix as $\vec v = \vec w$ in this case. \\
}
 
$\mathbf S^a$ is the square root of $\mathbf P^a$:

\begin{equation}
\mathbf P^a = \mathbf S^a {\mathbf S^a}^T
\end{equation}

Based on $\mathbf x^a$ and $\mathbf S^a$, an ensemble can be reconstructed:

\begin{equation}
{\mathbf x^a}^{(k)} = \mathbf x^a + {\sqrt{N-1}} \; {\mathbf S^a}^{(k)}
\end{equation}

The bias aware analysis scheme of \citet{dee98} is also implemented. But the error space $\mathbf S^a$ is not computed.

\subsection{Configuration}\label{sec_config}

The initialisation file of the assimilation module is composed mainly
by four sections: configuration of \textbf{the model} (model state vector,
position of the individual variables, error space of the model),
\textbf{observations} to assimilate (observations, their position, their
error), eventual \textbf{diagnostics} of the analysis and
miscellaneous \textbf{flags}.

\subsubsection{The model}

The following code contains the definition of the multivariate state
vector. The key \code{Model.variables} is a vector of character
strings attributing to each variable a user chosen name. The keys
\code{Model.gridX}, \code{Model.gridY}, \code{Model.gridZ} and
\code{Model.mask} are vectors of file names. The files in \code{Model.gridX} and 
\code{Model.gridY} are degenerated and give the longitude and latitude
of each variable. The files in \code{Model.gridZ} can be plain files
and contains the depth. The key \code{Model.mask} is used to determine
the sea-land mask of each variable. The exclusion value (or missing
value or \_FillValue in NetCDF terminology) marks a land point all other values, a sea
points. The arrays in \code{Model.gridX}, \code{Model.gridY}, \code{Model.gridZ} cannot 
contain an exclusion value/fill value.

Every files assembled into a state vector should have physical
values where mask assumes a sea point. The shape of the arrays in
\code{Model.gridX}, \code{Model.gridY}, \code{Model.gridZ} and
\code{Model.mask} must be the same. In some obvious cases, OAK can automatically repeat the coordinates values. For example is the mask has the size 10x11x12, then a latitude vector with 10 elements is automatically repeated along the 2nd and 3rd dimension.



The string in \code{Model.path} in prepended to each file names.
Example:


\begin{verbatim}
Model.variables = ['ETA'             ,'TEM'    ,'SAL'] 
Model.gridX     = ['ligur.X(:,:,end)','ligur.X','ligur.X'] 
Model.gridY     = ['ligur.Y(:,:,end)','ligur.Y','ligur.Y'] 
Model.gridZ     = ['ligur.Z(:,:,end)','ligur.Z','ligur.Z'] 
Model.mask      = ['ligur.Z(:,:,end)','ligur.Z','ligur.Z'] 
Model.path      = '/u/abarth/soft/Ligur3/Data/' 
\end{verbatim}

{%\color{color_nesting}
For nested grids the variables of the same nested must be grouped
and the groups must be orders according to the resolution started with
the highest resolution one. To each model grid is associated a
\code{Model.gridnum}: one for the highest resolution one, two of the
next highest resolution one and so one.

\begin{verbatim} 
Model.variables = ['TEM'    ,'SAL'    ,'TEM',   'SAL'] 
Model.gridX     = ['ligur.X','ligur.X','med.X','med.X'] 
Model.gridY     = ['ligur.Y','ligur.Y','med.Y','med.Y'] 
Model.gridZ     = ['ligur.Z','ligur.Z','med.Z','med.Z'] 
Model.mask      = ['ligur.Z','ligur.Z','med.Z','med.Z'] 
Model.gridnum   = [        1,        1,      2,      2] 
Model.path      = '/u/abarth/soft/Ligur3/Data/' 
\end{verbatim}}

\subsubsubsection{Mandatory keys}

\begin{keytabular}
\code{Model.mask} & vector of strings &  sea-land mask of each variable 
\\
\code{Model.gridX} & vector of strings & longitude of each variable (cannot contain missing values)
\\
\code{Model.gridY} & vector of strings & latitude of each variable  (cannot contain missing values)
\\
\code{Model.gridZ} & vector of strings & depth  (cannot contain missing values)
\\
\code{Model.path} & string & The path is prepended to all filenames
specified in \code{Model.*}. The current path is used by default.
\\
\code{ErrorSpace.dimension} & integer & The dimension of the error space.
\\
\code{ErrorSpace.init} & vector of strings &
Each string is a Fortran format containing an integer descriptor. The format is converted into a
file name with an internal write. The integer is a number ranging from 1
to the dimension of the error space $n$. $n$ vectors of file names are
formed and represent a error mode in the state space. Their norm
represent the importance of the error mode and thus they are in
general not normed. Orthogonality is not necessary.
\\
\code{ErrorSpace.type} & integer &  vectors are anomalies (type = 1) or vectors are ensemble members (with mean; type = 2)
\\
\code{ErrorSpace.scale} & float & inflation factor (default is 1, i.e. no inflation)
\\
\hline
\end{keytabular}

\subsubsubsection{Optional keys}

\begin{keytabular}
\code{Model.hres} & vector of real numbers & the resolution of the model grid which defines the ``priority'' when using overlapping grids. Observations are associated to the model variable with the highest resolution. The units of the resolution is not important as their values are only compared to each other (optional)
\\
\code{ErrorSpace.path} & string &  The path is prepended to all file names
specified in \code{ErrorSpace.*}. The current path is used by default.
\\
\code{ErrorSpace.scale} & real &
Each error mode is multiplied by this real number. The default is 1.
\\
\code{ErrorSpace.spaceScale} & vector of strings &
Each error mode is multiplied element by element by this vector. The
default is a vector with all elements equal to 1.
\\
\hline
\end{keytabular}

\subsubsection{Zones}

When the local version of the assimilation algorithm (\code{schemetype} = 1) is used, then the assimilation is performed in a number of zones independently. Zones are defined by specifying a partition vector which has the same number of variables as the model state vector and each variable has the same size as the corresponding \code{Model.mask}. This vector contains only integer values starting with one and represent labels: all elements in the state vector which have the same partition number belong to the same zone. For example, for a state vector with 5 elements and the partition vector $\mathbf p$:

\begin{equation}
\mathbf x = 
\left( 
\begin{array}{c}
x_1 \\
x_2 \\
x_3 \\
x_4 \\
x_5
\end{array}
\right) 
\qquad
\mathbf p = 
\left( 
\begin{array}{c}
1 \\
1 \\
2 \\
2 \\
3
\end{array}
\right) 
\end{equation}

This partition vector defined three zones: the first zone contains elements $x_1$ and $x_2$, the second zone $x_3$ and $x_4$ and the third zone $x_3$. There should be no gaps in the partition vector. For example the vector $(1,1,2,2,4)^T$ would cause an error. 
In practice, the state vector is partitioned along water columns.
The assimilation is performed independently in each zone using only observations within the search radius given by 
\code{Zones.maxLength}. The weight of the observations $\frac{1}{R'}$ is multiplied by a Gaussian function:
\begin{equation}
\frac{1}{R'} = \frac{1}{R'} \exp(- (d/L)^2)
\end{equation}

where $d$ is the horizontal distance (in m) the first point of a zone and a single observation and $L$ a length-scale (in m) given by \code{Zones.corrLength}. \code{Zones.maxLength} and \code{Zones.corrLength} have the same size as the model state vector. In most cases these values are constant can be specified by, e.g.:

\begin{verbatim} 
Zones.corrLength.const = [           30e3,             30e3]
Zones.maxLength.const  = [         2000e3,           2000e3]
\end{verbatim}

\begin{keytabular}
\code{Zones.partition} & vector of strings & Each string is a file name containing
  the partition file for the given model variable
\\
\code{Zones.corrLength} & vector of strings & Each string is a file name containing 
  the correlation length
\\
\code{Zones.maxLength} & vector of strings & Each string is a file name containing 
  the maximum correlation length
\\
\hline
\end{keytabular}


\subsubsection{The observations}

All set of simultaneous observation are ordered chronically and are attributed to a time
index starting with 001 (written always with three digits). In the
following keys ``XXX'' have to be replaced by the time index. 


\subsubsubsection{Mandatory keys}

\begin{keytabular}
\code{ObsXXX.time} & 'yyyy-mm-ddTHH:MM:ss' &
  yyyy=year  (minimum 1 digit integer)
  mm=month (2 digits integer)
  dd=day   (2 digits integer)
  HH=hour (2 digits integer)
  MM=min  (2 dig-ids integer)
  SS=second (minimum 1 digit integer or real)
\\  
\code{ObsXXX.value} & vector of strings & Each string is a file name containing
  the actual values of the observations
\\
\code{ObsXXX.mask} & vector of strings & Each string is a file name containing 
  the binary mask of the observations. Values where the mask is different
  from 1 are rejected.
\\
\hline
\end{keytabular}

\subsubsubsection{Diagonal observation error covariance matrix}

\begin{keytabular}
\code{ObsXXX.rmse} & vector of strings & Each string is a file name containing 
  the root mean square error of the observations.
\\
\hline
\end{keytabular}

\subsubsubsection{Non-Diagonal observation ($\mathbf B + \mathbf C \mathbf C^T$)}

All keys start with \code{ObsXXX.SMWCovar}.

\begin{keytabular}
\code{SqrtDiag} & vector of strings & Each string is a file name containing the root of the diagonal elements of $\mat B$.
\\
\code{dimension} & integer & The dimension of the error space $\mat C$.
\\
\code{ErrorSpace} & vector of strings & 
Each string is a Fortran format containing an integer descriptor. The format is converted into a
file name with an internal write. The integer is a number ranging from 1
to the dimension of the error space $n$. $n$ vectors of file names are
formed and represent a error mode in the state space. Their norm
represent the importance of the error mode and thus they are in
general not normed. Orthogonality is not necessary.
\\
\code{type} & integer &  vectors in \code{ErrorSpace} are anomalies (type = 1) or vectors are ensemble members (with mean; type = 2). In the later case, the mean will be ignored.
\\
\code{scale} & float & inflation factor (default is 1, i.e. no inflation)
\\
\hline
\end{keytabular}


\subsubsubsection{Optional keys}

\begin{keytabular}
\code{ObsXXX.variables} & vector of strings & The names must correspond to the names 
  chosen in Model.variables. Unknown names are treated as "out of the grid"
  and are not assimilated.
\\
\code{ObsXXX.names} & vector of strings & Each string is a description of 
the data type of the observations. You can choose any name meaningful 
to you. These names are only used for the log-file. The default names 
are \code{Var01}, \code{Var02},...
\\
\code{ObsXXX.gridX} & vector of strings & Each string is a file name containing 
  the longitude of the observations.
\\
\code{ObsXXX.gridY} & vector of strings & Each string is a file name containing 
  the latitude of the observations.
\\
\code{ObsXXX.gridZ} & vector of strings & Each string is a file name containing 
  the depth of the observations.
\\
\code{ObsXXX.HperObs} & vector of strings & The observation operator stored in a 
   sparse matrix form per observations
\\
\code{ObsXXX.operator} & string & The observation operator stored in a 
   sparse matrix form.   
\\
\code{ObsXXX.path} & string & The path is prepended to all file names
specified in \code{ObsXXX.*}. The current path is used by default.
\\
\hline
\end{keytabular}

The optional keys are used to create the observation operator. If it
is applied to the state vector, it extracts the observed variables at
the location of the measurements. Several ways exist to specify the
observation operator.

\begin{enumerate}
\item \code{ObsXXX.operator}: the observation operator is directly
  given by the non zero elements.  See
  also \ref{format_oper}. 
\item \code{ObsXXX.variables} and \code{ObsXXX.HperObs}: the non zero
  elements of the observation operator for each variable are given
  separately. The first column in $9 \times x$ matrix is ignored. See
  also \ref{format_oper}. 
\item \code{ObsXXX.variables}, \code{ObsXXX.gridX},
  \code{ObsXXX.gridY} and \code{ObsXXX.gridZ}: the observation
  operator is created by a trilinear interpolation using the module \code{grids}. 
\end{enumerate}   

Note that the individual arrays in \code{ObsXXX.value}, \code{ObsXXX.rmse}, \code{ObsXXX.mask}, \code{ObsXXX.gridX},
\code{ObsXXX.gridY} and \code{ObsXXX.gridZ} should have the same size.

\subsubsubsection{Format of the observation operator}\label{format_oper}

Only the non-zero elements of the observation operator are specified
in the $9 \times n$ matrix (in column-major order) where n is the number of non-zero
elements. Each column has the following structure:

\begin{tabular}{*{4}{|p{.085\textwidth}}|*{4}{|p{.085\textwidth}}||p{.08\textwidth} |}  
\hline 
\multicolumn{4}{|c||}{Observations} & 
\multicolumn{4}{c||}{Model} &  \\
\hline \hline 
var. index &
i-index &
j-index &
k-index &
var. index &
i-index &
j-index &
k-index &
Inter-polation coefficient \\
\hline
\end{tabular}

The first integer value is related to the observation. The index of
the variable is the position where the observed variable appears 
in \code{ObsXXX.value} and i,j,k-index are the three spatial indexes
of a single scalar observation. \\

The integers in column 5 to 8
are related to the model state vector. Again the index of
the variable is the position where the observed variable appears 
in \code{Model.variables} and  i,j,k-index are the three spatial indexes
of a single scalar model forecast.
If one of the model indexes is -1 the corresponding observation is
treated "out of grid" and the associated weight will be zero.

The column 9 is a real value between 0 and 1 in the case of a simple
trilinear interpolation. 
The observation operator can be generated offline using a trilinear interpolation 
with the tool "\code{genobsoper}".

\subsubsection{Diagnostics}

All diagnostics are optional and the corresponding files are output.

\begin{keytabular}
\code{DiagXXX.xf} & vector of strings & the model forecast (ensemble mean)
\\
\code{DiagXXX.Hxf} & vector of strings & the observed part of the model forecast
\\
\code{DiagXXX.Sf} & vector of strings & Each string is a Fortran
format. For the conversion into file names see the key
\code{ErrorSpace.init}. The files represent
the error modes of the model forecast.
\\
\code{DiagXXX.Ef} & vector of strings & Each string is a Fortran
format. For the conversion into file names see the key
\code{ErrorSpace.init}. The files represent
the forecast ensemble.
\\
\code{DiagXXX.diagPf} & vector of strings & The diagonal elements of error
covariance of the model forecast.
\\
\code{DiagXXX.diagHPfHT} & vector of strings & The diagonal elements of error
covariance of the observed part of the model forecast 
\\
\code{DiagXXX.stddevxf} & vector of strings & Standard deviation of
the error of the model forecast.
\\
\code{DiagXXX.stddevHxf} & vector of strings & Standard deviation of
the error of the observed part of the model forecast.
\\
\code{DiagXXX.path} & string & The path is prepended to all file names
specified in \code{DiagXXX.*}. The current path is used by default.
\\
\hline
\code{DiagXXX.xa} & vector of strings & the analysis (ensemble mean)
\\
\code{DiagXXX.Hxa} & vector of strings & the observed part of the analysis
\\
\code{DiagXXX.Sa} & vector of strings & Each string is a Fortran
format. For the conversion into filenames see the key
\code{ErrorSpace.init}. The files represent
the error modes of the analysis.
\\
\code{DiagXXX.Ea} & vector of strings & Each string is a Fortran
format. For the conversion into file names see the key
\code{ErrorSpace.init}. The files represent
the analysis ensemble.
\\
\code{DiagXXX.diagPa} & vector string & The diagonal elements of error
covariance of the analysis.
\\
\code{DiagXXX.diagHPaHT} & vector of strings & The diagonal elements of error
covariance of the observed part of the analysis 
\\
\code{DiagXXX.stddevxa} & vector of strings & Standard deviation of
the error of the analysis.
\\
\code{DiagXXX.stddevHxa} & vector of strings & Standard deviation of
the error of the observed part of the analysis.
\\
\hline
\code{DiagXXX.H} & strings & the observation operator
\\
\code{DiagXXX.yo} & vector of strings & The observations.
\\
\code{DiagXXX.invsqrtR} & vector of strings & The inverse of the root mean
square error of the observations. If a scalar observation point has
been eliminated (out of the model grid for example) its weight is zero.
\\
\code{DiagXXX.xa-xf} & vector of strings & The analysis increment
\\
\code{DiagXXX.yo-Hxf} & vector of strings & the observation minus the
model forecast at the observation points
\\
\code{DiagXXX.yo-Hxa} & vector of strings & the observation minus the
model analysis at the observation points
\\
\code{DiagXXX.Hxa-Hxf} & vector of strings & analysis increment 
at the observation points
\\
\code{DiagXXX.path} & string & The path is prepended to all filenames
specified in \code{DiagXXX.*}. The current path is used by default.
\\
\hline
\end{keytabular}

\subsubsection{miscellaneous flags}

\begin{keytabular}
\code{nbnest} & integer &  Number of nested grids 
\\
\code{assimnum} & integer & Number between 1 and \code{nbnest}
different for each model. The model with \code{assimnum} does the assimilation
\\
\code{runtype} & integer & 
possible values of \code{runtype} are:
\begin{description}
\item[0:] do nothing, i.e. a pure run of the model  
\item[1:] still do not assimilate, but compare model to observations
\item[2:] assimilate observations
\end{description}
\\
\code{schemetype} & integer &
possible values of \code{schemetype} are:
\begin{description}
\item[0:] global assimilation (default)
\item[1:] local assimilation (\code{Zones} need to be defined)
\end{description}
\\
\code{moderrtype} & integer &
possible values of \code{moderrtype} are:
\begin{description}
\item[0:] optimal interpolation Pf constant
\item[1:] forgetting factor approximation
\end{description}
\\
\code{biastype} & integer &
possible values of \code{biastype} are: 
\begin{description}
\item[0:] standard bias-blind analysis
\item[1:]  A fraction of the error (gamma) is a systematic error 
and the rest (1-gamma) is random \citep{dee98}
\end{description}
\\
\code{Bias.gamma} & real & fraction of the error with is systematic
\\
\code{Bias.init} & vector of string & the initial estimation of the bias
\\
\code{joinvectors} & integer & If joinvectors is 1 then the variables of the nested grids will be assembled to one multigrid state vector
\\
\code{logfile} & string & File contains simple diagnostics such as rmse with observations 
\\
\code{debugfile} & string & 
File contains debugging information is the code was compiled with the flag \code{-DDEBUG}
\\
\hline
\end{keytabular}


\section{Data structures}

The subroutine assim requires as an argument a part of the model state for local assimilation. The way the data is distributed can be explained by the following steps:

\begin{enumerate}
\item for each variable concatenate the model sub-domains (if the model domain is decomposed into sub-domains)
\item concatenate all variables
\item remove masked elements
\item permute the order of the elements so that all elements belong to the same zone are continuous in memory. The elements are ``sorted'' using numeric labels in the partition vector (the sort is stable, i.e. if two elements have the same partition label, their order is not changed).
\item each vector is distributed among the available processes
\end{enumerate}

The actual implements avoid to form a global vector spanning the entire state vector and goes directly from the first step to the last.

\section{Standalone programs}\label{sec_standalone}

\subsection{Program \code{assim}}\label{prog_assim}

The standalone program \code{assim} can be used to test the assimilation. The program can be called from the command line:

\begin{verbatim}
assim <initfile> <time index>
\end{verbatim}

The first argument is the initialisation file and the second argument is the time index of the observation to assimilate. All keys described in \ref{sec_config} have the same meaning for the program \code{assim}. But the forecast has to be specified by the following keys.

\begin{keytabular}
\code{ForecastXXX.value} & vector of strings & the forecast
\\
\code{ForecastXXX.path} & string &  The path is prepended to all filenames
specified in \code{ForecastXXX.value}. The current path is used by default.
\\
\hline
\end{keytabular}

If the program is called with three arguments:

\begin{verbatim}
assim <initfile> <start time index>  <end time index>
\end{verbatim}

All assimilation cycles be between the two time indexes are performed in chronological order.

\subsection{Program \code{genobsoper}}

The standalone program \code{genobsoper} generate the observation matrix.

\begin{verbatim}
genobsoper <initfile> <time index>
\end{verbatim}

The first argument is the initialisation file and the second argument is the time index of the observation for witch the observation operator has to be created. All keys described in \ref{sec_config} have the same meaning for the program \code{genobsoper}. But the only diagnostic key used is \code{DiagXXX.H}.

If the program is called with three arguments:

\begin{verbatim}
genobsoper <initfile> <start time index>  <end time index>
\end{verbatim}

The action is repeated for all time indexes between the start and the end time index.

\subsection{Program \code{applyobsoper}}

The standalone program \code{applyobsoper} extracts from a state vector the observations.

\begin{verbatim}
applyobsoper <initfile> <time index>
\end{verbatim}

The first argument is the initialisation file and the second argument is the time index of the observation for witch the observation operator has to be created. All keys described in \ref{sec_config} have the same meaning for the program \code{applyobsoper}. But the only diagnostic key used are \code{DiagXXX.Hxf} and \code{DiagXXX.invsqrtR}. If a scalar observation point has
been eliminated (out of the model grid for example) its weight in \code{DiagXXX.invsqrtR} is zero. The state vector is specified as it is described in \ref{prog_assim}.

If the program is called with three arguments:

\begin{verbatim}
applyobsoper <initfile> <start time index>  <end time index>
\end{verbatim}

The action is repeated for all time indexes between the start and end time index.

\subsection{Program \code{filteroper}}

The standalone program \code{filteroper} generates a sparse matrix witch acts as a spatial filter in the model space. 

\begin{verbatim}
filteroper <initfile>
\end{verbatim}

For each variable the filter is a Gaussian function:

\begin{equation} \label{eqn_filter}
f(x,y,z,x',y',z') = N e^{-\frac{(x-x')^2}{L_x^2}-\frac{(y-y')^2}{L_y^2}-\frac{(z-z')^2}{L_z^2}}
\end{equation}

$N$ is a normalisation factor taking in to account the land-sea mask. The parameters $L_x$, $L_y$ and $L_z$ may be space dependent and have thus the same dimension as the state vector.
The required keys are: \\

\begin{keytabular}
\code{Model.mask} & vector of strings &  sea-land mask of each variable 
\\
\code{Model.gridX} & vector of strings & longitude of each variable (without any exclusion value/fill value)
\\
\code{Model.gridY} & vector of strings & latitude of each variable (without any exclusion value/fill value)
\\
\code{Model.gridZ} & vector of strings & depth (without any exclusion value/fill value)
\\
\code{Model.hres} & vector of real numbers & the resolution of the model grid which defines the ``priority'' when using overlapping grids. Observations are associated to the model variable with the highest resolution. The units of the resolution is not important as their values are only compared to each other (optional)
\\
\code{Model.path} & string & The path is prepended to all filenames
specified in \code{Model.*}. The current path is used by default.
\\
\code{Correlation.lenx} & vector of strings & parameter $L_x$ in equation \ref{eqn_filter}
\\
\code{Correlation.leny} & vector of strings & parameter $L_y$ in equation \ref{eqn_filter}
\\
\code{Correlation.lenz} & vector of strings & parameter $L_z$ in equation \ref{eqn_filter}
\\
\code{Correlation.path} & string & The path is prepended to all filenames
specified in \code{Correlation.*}. The current path is used by default.
\\
\code{Filter} & string & file name of the filter
\\
\hline
\end{keytabular}

\subsection{Program \code{opermul}}

\code{opermul} is a general purpose program witch multiply two sparse operators. It can be used for example for multiplying a filter operator and a observation operator.

\begin{equation}
{\cal O}_3 = {\cal O}_2 {\cal O}_1  
\end{equation}

${\cal O}_1$ is a operator mapping from space $S_1$ to $S_2$, ${\cal O}_2$ from $S_2$ to $S_3$ and thus the product from  $S_1$ to $S_3$.
 
\begin{verbatim}
opermul <initfile>
\end{verbatim}

The required keys are: \\

\begin{keytabular}
\code{Space1.mask} & vector of strings &  sea-land mask of space $S_1$ 
\\
\code{Space1.path} & string & The path is prepended to all filenames
specified in \code{Space1.mask}. The current path is used by default.
\\
\code{Space2.mask} & vector of strings &  sea-land mask of space $S_1$ 
\\
\code{Space2.path} & string & The path is prepended to all filenames
specified in \code{Space2.mask}. The current path is used by default.
\\
\code{Space3.mask} & vector of strings &  sea-land mask of space $S_1$ 
\\
\code{Space3.path} & string & The path is prepended to all filenames
specified in \code{Space2.mask}. The current path is used by default.
\\
\code{Term1} & string & file name of operator ${\cal O}_1$
\\
\code{Term2} & string & file name of operator ${\cal O}_2$
\\
\code{Product} & string & file name of the product ${\cal O}_3$
\\
\hline
\end{keytabular}

\subsection{Matlab utility GenObsFile}

The utility "GenObsFile" provides an easy way to save all the observations, coming from various sources, in a few files with the NetCDF format, and creates the .INIT file required by the assimilation routines.

Options for GenObsFile must be specified in the header of the Matlab routine, as described below:

\begin{itemize}
\item initheader: complete path \& file name, of the file that must be copied on top of the .INIT file. This could be the "model" part of the .INIT file.

\item diags: complete path \& file of a sample "diagnostic" part of the .INIT file. The observation number should be replaced with $<$INDEX$>$ and variable names with $<$EXT$>$. This part will be (adapted and) copied for each 
observation set.

Example: 
\begin{verbatim}
Diag<INDEX>.Hxf =  ['xf.<EXT>']
\end{verbatim}

\item Outdir : path where to store the new observations and .INIT file.

\item Outfile : prefix of the new observation files

\item maxX, minX, maxY, ..., minMJD: observations not within these ranges will be ignored when creating the new observation files

\item rmse : vector containing errors on the observations, in the
  following order:

\begin{verbatim}
[TEM SAL ETA other]=[...]
\end{verbatim}
It will only be used by the assimilation routine if no other observation error covariance $\mathbf R$ matrix is specified. GenObsFile only uses values corresponding to variables present in your observations list.

\item obstime : time of the day at which observations should be assimilated

\item listfile : complete path+file name for the listfile, which contains the original observations. It is build using sections. There must be at least one section in the listfile. Each section contains a "config" line followed by an arbitrary amount of data lines.
The config line starts with the keyword 'config', and has the
following format: config VAR X Y Z MJD\\
\begin{itemize}
\item VAR indicates how the observed data should be named in the .INIT file (TEM ...)\\
\item X might be (a) a complete path+file name with the longitude data, corresponding to the observations, (b) the keyword 'file' if the longitude data is written in a file with the same file name as the actual data, with extension .X\\
\item Y (idem)\\
\item Z (idem)\\
\item MJD points to the file containing the MJD-time corresponding to the
\item observations, and might be (a) a complete path+file name, (b) the
\item keyword 'file', (c) a datum in the format 1999-12-31, (d) a datum in
\item the MJD format '51251', (e) character limits to be found in the
\item actual observations file name.\\
\end{itemize}
For example, if the actual file name is /home/johndoe/51657.TEM , MJD could be 15-19 as those are the indexes pointing to 51657 in the file name.
After each config line, an arbitrary amount of observation files may be given.
The filenames may contain matrix delimiters, as in (1:100,2:5,:)\\
\end{itemize}

Example listfile:
\begin{verbatim}
config TEM ./Lion.X ./Lion.Y ./Lion.Z 1998-01-01
/home/johndoe/observations/Lion00000480.TEM.gz(:,:,end)
config SAL ./Lion.X ./Lion.Y ./Lion.Z 1998-01-01
/home/johndoe/observations/Lion00000480.SAL.gz(:,:,end)
config TEM file file file file
/home/johndoe/observations/ctd02.1_03_aug_2241.TEM
/home/johndoe/observations/ctd03.1_03_aug_1840.TEM
/home/johndoe/observations/ctd04.1_04_aug_0747.TEM
config TEM ./ligur.SST.X ./ligur.SST.Y ./ligur.SST.Z 32-41
/scratch/johndoe/observtn/ligur1999-07-02.SST.gz
/scratch/johndoe/observtn/ligur1999-07-03.SST.gz
/scratch/johndoe/observtn/ligur1999-07-04.SST.gz
/scratch/johndoe/observtn/ligur1999-07-10.SST.gz
/scratch/johndoe/observtn/ligur1999-07-11.SST.gz
\end{verbatim}

\section{API}
\subsection{ufileformat}

\begin{description}
\item[\code{uload(filename,matrix,exclusion\_value)}] ~\newline
\begin{description}
\item[\code{filename}]: character of strings, input. The filename of
 the matrix to load with
  the extensions described in \ref{subsec_use_ufileformat}.
\item[\code{matrix}]: 1D, 2D or 3D unallocated real pointer, output. The
  allocation of the output matrix is done inside the subroutine.
\item[\code{exclusion\_value}]: real, output: The exclusion value
\end{description}
\item[\code{usave(filename,matrix,exclusion\_value)}] ~\newline
\begin{description}
\item[\code{filename}]: character of strings, input. The filename of
 the matrix to save.
\item[\code{matrix}]: 1D, 2D or 3D real matrix, input. The matrix to
  save.
\item[\code{exclusion\_value}]: real, input: The exclusion value
\end{description}

\end{description}


\bibliographystyle{plainnat}
\bibliography{all}

\end{document}

% LocalWords:  Luc ufileformat GHER initfile matoper BLAS LAPACK rrsqrt dainit
% LocalWords:  daobs daanalysis damoderr gunzip TMPDIR tmp toto TEM indices dee
% LocalWords:  runtype Burne gmatch ESSE gridX gridY gridZ gridnum ErrorSpace
% LocalWords:  init spaceScale ObsXXX subsubsubsection yyyy HH rmse HperObs xf
% LocalWords:  trilinear genobsoper DiagXXX Hxf diagPf diagHPfHT stddevxf xa Sa
% LocalWords:  stddevHxf Hxa applyobsoper filteroper lenx leny lenz opermul MJD
% LocalWords:  GenObsFile initheader diags Outdir Outfile maxX minX maxY minMJD
% LocalWords:  obstime listfile config API uload filename usave barth beckers
% LocalWords:  rixen ncdump eigenvector
